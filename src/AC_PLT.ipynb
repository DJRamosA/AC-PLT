{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPN generator\n",
    "\n",
    "## Table of contents\n",
    "1. [Libraries](##Libraries)\n",
    "2. [Important Variables](##important-variables)\n",
    "3. [Language Setting](#language-setting)\n",
    "4. [Important functions](##important-functions)\n",
    "5. [Data Cleaning](#data-cleaning)\n",
    "6. [Word Embedding](##word-embedding)\n",
    "7. [Model](#model)\n",
    "8. [Codification Suggested](#codification-suggested)\n",
    "\n",
    "## [Libraries](##Libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install all the requirement\n",
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import gensim\n",
    "from collections import Counter \n",
    "import sklearn.cluster\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "\n",
    "# Set of the random seed for the kmeans model\n",
    "random.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Important Variables](##important-variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "# This the path to run the main example, for your data change the path to your data\n",
    " \n",
    "pathTrainData = r'../data/CPN27/CPN27.csv'              # Path of the training data\n",
    "pathData = r'../data/CPN27/CPN27-to-Codify.csv'         # Path of the data to codify\n",
    "pathEmbedding = r'../data/SBW-vectors-300-min5.bin.gz'  # For example in Spanish\n",
    "outputFile = r'../data/ACPLT-output.csv'                # Path for the results of the AC-PLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "numberCluster = 500 # Number of clusters for the k-means model\n",
    "numberCodes = 3 # Number of code you want as suggestion.\n",
    "language = \"spanish\" # You can change to \"english\"\n",
    "embeddingLength = 300 # Change by the length of the embedding to be used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Language Setting](#language-setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "# Relative Path of the dataset, change for your dataset\n",
    "data_train = pd.read_csv(pathTrainData, delimiter=\",\")\n",
    "\n",
    "# Import of the data to codify\n",
    "data_to_code = pd.read_csv(pathData, delimiter=\",\")\n",
    "\n",
    "\n",
    "if language==\"spanish\":\n",
    "    # Import of the model of the spanish billion words embeddings\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format( \n",
    "    pathEmbedding, # Relative path of the vector\n",
    "    binary=True # The model is in binary format\n",
    "    ) \n",
    "else:\n",
    "    # Import of the model of the word2vec-google-news-300 for English dataset\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format( \n",
    "    pathEmbedding, # Relative path of the vector\n",
    "    binary=False # The model is in binary format\n",
    "    )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra (concepto)</th>\n",
       "      <th>Descripción</th>\n",
       "      <th>Codificación</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compasión</td>\n",
       "      <td>sentimiento</td>\n",
       "      <td>sentimiento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compasión</td>\n",
       "      <td>por lo que se le perdona la vida a alguien en ...</td>\n",
       "      <td>perdón</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plan</td>\n",
       "      <td>necesario para cumplir objetivos</td>\n",
       "      <td>objetivos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plan</td>\n",
       "      <td>organizar recursos o personas</td>\n",
       "      <td>organización</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plan</td>\n",
       "      <td>estrategia</td>\n",
       "      <td>estrategia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Palabra (concepto)                                        Descripción  \\\n",
       "0         Compasión                                         sentimiento   \n",
       "1         Compasión   por lo que se le perdona la vida a alguien en ...   \n",
       "2               plan                   necesario para cumplir objetivos   \n",
       "3               plan                      organizar recursos o personas   \n",
       "4               plan                                         estrategia   \n",
       "\n",
       "   Codificación  \n",
       "0   sentimiento  \n",
       "1        perdón  \n",
       "2     objetivos  \n",
       "3  organización  \n",
       "4    estrategia  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the model to work correctly, it needs 3 columns (Concept, Feature, and Code in that order), the name itself is not important.\n",
    "# Code is important for the training of the model.\n",
    "data.head(5) # Example with used dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of the Spanish pipeline. Change depending on the language you are using it.\n",
    "\n",
    "More info in the page of spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download each one depending on your language preference\n",
    "\n",
    "if language==\"spanish\":\n",
    "    !python -m spacy download es_core_news_sm    \n",
    "else:\n",
    "    !python -m spacy download en_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of spacy\n",
    "if language==\"spanish\":\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "else:\n",
    "    nlp = spacy.load('en_core_news_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Important functions](##important-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Receive a string and return tokens of each word, without punctuations\n",
    "    and in lowercase\n",
    "    \"\"\"\n",
    "    # for each token in the sentence, add the token to the list if is not a punctuation\n",
    "    return [t for t in nlp(text.lower()) if not t.is_punct]\n",
    "\n",
    "\n",
    "def normalize(tokens):\n",
    "    \"\"\"\n",
    "    Receive al list of tokens and return in one string without stop words\n",
    "    \"\"\"\n",
    "    # for each token if is not a stop word add the word to the list\n",
    "    words = [t.orth_ for t in tokens if not t.is_stop]\n",
    "\n",
    "    # return the tokens in one string\n",
    "    return(\" \".join(words))\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    \"\"\"\n",
    "    Receive al list of tokens and return in one string without stop words \n",
    "    and Lemmatized\n",
    "    \"\"\"\n",
    "    # for each token if is not a stop word add the lemma of the word in the list\n",
    "    lemmas = [t.lemma_ for t in tokens if not t.is_stop]\n",
    "\n",
    "    # return the tokens in one string\n",
    "    return(\" \".join(lemmas))\n",
    "\n",
    "\n",
    "def to_vector(texto, model):\n",
    "    \"\"\" \n",
    "    Receives a sentence string along with a word embedding model and \n",
    "    returns the vector representation of the sentence\n",
    "    \"\"\"\n",
    "    tokens = texto.split() # splits the text by space and returns a list of words\n",
    "    vec = np.zeros(embeddingLength) # creates an empty vector of 300 dimensions\n",
    "    for word in tokens: # iterates over the sentence\n",
    "        if word in model: # checks if the word is both in the word embedding\n",
    "            vec += model[word] # adds every word embedding to the vector\n",
    "    return vec / np.linalg.norm(vec) if np.linalg.norm(vec)>0 else vec # divides the vector by their normal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Data Cleaning](##data-cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceution time: 23.832730293273926\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Normalize of the feature\n",
    "for i in range(len(data_train)):\n",
    "    data_train.iloc[i,1] = normalize(clean_text(data_train.iloc[i,1])) # Change for the name of your dataset\n",
    "\n",
    "for i in range(len(data_to_code)):\n",
    "    data_to_code.iloc[i,1] = normalize(clean_text(data_to_code.iloc[i,1])) # Change for the name of your dataset\n",
    "end = time.time()\n",
    "print(\"Execution time:\", end-start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Word Embedding](##word-embedding)\n",
    "\n",
    "Word Embedding for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceution time: 0.7288105487823486\n"
     ]
    }
   ],
   "source": [
    "# Timer\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Creation of the matrix to make the clustering process\n",
    "descriptions_matrix = np.zeros( # creatign an empty matrix\n",
    "    (\n",
    "        len(data_train), # the number of data points\n",
    "        embeddingLength  # the number of components of the word embedding\n",
    "    )\n",
    ")\n",
    "\n",
    "# Matrix filling \n",
    "# Change to the name of the descriptions of your dataset.\n",
    "for i,description in enumerate(data_train.iloc[:,1]):\n",
    "    vector = to_vector(description,model)\n",
    "    descriptions_matrix[i,] = vector\n",
    "\n",
    "# Concatenate the matrix with the data of each observation\n",
    "data_matrix = np.concatenate([descriptions_matrix,data_train], axis=1)\n",
    "\n",
    "\n",
    "# Remove of the 'Nan' data\n",
    "data_matrix = data_matrix[~pd.isnull(data_matrix[:,:300]).any(axis=1)]\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Execution time:\", end-start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding for the dataset to code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceution time: 0.00703120231628418\n"
     ]
    }
   ],
   "source": [
    "# Timer\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Creation of the matrix to make the clustering process\n",
    "descriptions_matrix = np.zeros( # creatign an empty matrix\n",
    "    (\n",
    "        len(data_to_code), # the number of data points\n",
    "        300                # the number of components of the word embedding\n",
    "    )\n",
    ")\n",
    "\n",
    "# Matrix filling of the dataset to codify\n",
    "for i,description in enumerate(data_to_code.iloc[:,1]):\n",
    "    vector = to_vector(description,model)\n",
    "    descriptions_matrix[i,] = vector\n",
    "\n",
    "# Concatenate the matrix with the data of each observation\n",
    "matrix_to_code = np.concatenate([descriptions_matrix,data_to_code], axis=1)\n",
    "\n",
    "\n",
    "# Remove of the 'Nan' values in the data\n",
    "matrix_to_code = matrix_to_code[~pd.isnull(matrix_to_code[:,:300]).any(axis=1)]\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Execution time:\", end-start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Model](#model)\n",
    "\n",
    "We create a class for the text classification, were initially you have to set the number of clusters you want to use for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC_PLT:\n",
    "\n",
    "    def __init__(self, n_clusters = 500, random_state=0):\n",
    "        \"\"\"\n",
    "        n_clusters: number of cluster in the k-Means model\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_clusters = n_clusters # number of clusters\n",
    "        self.KMeans_dict = {} # dictionary of all the humans codifications for each Cluster\n",
    "        self.KMeans_categories = {} # dictionary for the most frecuent value in the centroid\n",
    "        self.km = sklearn.cluster.KMeans(           # creates de k-means object\n",
    "            n_clusters=self.n_clusters, \n",
    "            random_state=random_state,\n",
    "            n_init='auto'\n",
    "        ) \n",
    "        \n",
    "        \n",
    "    def most_frequent(self, List): \n",
    "        \"\"\"\n",
    "        Receives a list of words, and return the word most frequente of\n",
    "        the list\n",
    "        \"\"\"\n",
    "        # counter of occurence of a code in a list\n",
    "        occurence_count = Counter(List) \n",
    "        \n",
    "        # Return the first code with more occurence\n",
    "        return occurence_count.most_common(1)[0][0] \n",
    "\n",
    "\n",
    "    def fit(self, train):\n",
    "        \"\"\"\n",
    "        Receives the train dataset and the number of clusters to train \n",
    "        the k-means model\n",
    "        \"\"\"\n",
    "        # Train the k-means algorithm\n",
    "        self.km.fit(train[:,:300])\n",
    "\n",
    "        # Dataframe of train dataset\n",
    "        df = pd.DataFrame(\n",
    "            np.concatenate([\n",
    "                np.reshape(train[:,302], (-1, 1)),          # Human codification\n",
    "                np.reshape(self.km.labels_, (-1, 1)),       # Number of the KMean centroid\n",
    "                np.reshape(train[:,300], (-1, 1))           # Concept of the codification\n",
    "                ], axis=1), \n",
    "            columns=['Human', 'KMeans', 'Concept'])\n",
    "\n",
    "        # create a dictionary of all the humans codifications for each Cluster\n",
    "        self.KMeans_dict = df.groupby(by='KMeans')['Human'].apply(list).to_dict()\n",
    "\n",
    "        # Fill a dictionary with the most frecuent value in the centroid\n",
    "        for key, val in self.KMeans_dict.items():\n",
    "            self.KMeans_categories[key] = self.most_frequent(val)\n",
    "        \n",
    "        # Generates the prediction for the train dataset\n",
    "        df['KM_Prediction'] = df['KMeans'].map(self.KMeans_categories)\n",
    "\n",
    "\n",
    "    def get_distances(self, test):\n",
    "        \"\"\"\n",
    "        Receives the test data to calculate the distances of each frase, return \n",
    "        a matrix with the distances sorted\n",
    "        \"\"\"\n",
    "        \n",
    "        # Distance matrix of each test point to each cluster center\n",
    "        distance_matrix = distance.cdist(test[:,:300].astype(float), self.km.cluster_centers_, 'euclidean')\n",
    "        \n",
    "        # Sorting distances\n",
    "        self.topk=np.argsort(distance_matrix,axis=1)\n",
    "        \n",
    "    \n",
    "    def set_labels(self):\n",
    "        \"\"\"\n",
    "        Create a new matrix from the clusters sorted and change the value\n",
    "        from numeric to the string according the codification\n",
    "        \"\"\"\n",
    "        # Change of the numeric value to the codification \n",
    "        self.topKS=pd.DataFrame(self.topk)\n",
    "\n",
    "        # create a temporal array of the kmeans categories\n",
    "        tempData = np.array([value for (_, value) in sorted(self.KMeans_categories.items())])\n",
    "        \n",
    "        # print(tempData)\n",
    "\n",
    "        # for each cluster center\n",
    "        for j in range(self.topKS.shape[1]):\n",
    "            # set the codification of the numeric value in the topk list\n",
    "            self.topKS.iloc[:,j]=tempData[self.topk[:,j]]\n",
    "\n",
    "\n",
    "    def get_accuracies(self, test):\n",
    "        \"\"\"\n",
    "        Receives the test matrix and return the accuracies of the \n",
    "        diferents predictions\n",
    "        \"\"\"\n",
    "        #Creating the accuracy table to check each data point\n",
    "        testLabel=np.zeros(self.topKS.shape)\n",
    "        indexes_method0=pd.DataFrame(np.zeros((self.topKS.shape[0],2)), columns=['index', 'value']) \n",
    "\n",
    "        #For each data point\n",
    "        for i in range(testLabel.shape[0]):\n",
    "            #Checking if some of the cluster is able to classify it right\n",
    "            boolClass=self.topKS.iloc[i,:]==test[i,302]\n",
    "            if sum(boolClass)>0:\n",
    "                getIndex=boolClass.idxmax()\n",
    "                indexes_method0.iloc[i,0] = getIndex\n",
    "                indexes_method0.iloc[i,1] = self.topKS.iloc[i,getIndex]\n",
    "                #Setting the rest of the data point as 1\n",
    "                testLabel[i,getIndex:]=1\n",
    "            else:\n",
    "                indexes_method0.iloc[i,0] = np.nan\n",
    "                indexes_method0.iloc[i,1] = np.nan\n",
    "        accuracies=testLabel.sum(axis=0)/testLabel.shape[0]\n",
    "\n",
    "        return accuracies\n",
    "\n",
    "    \n",
    "    def transform(self, test):\n",
    "        \"\"\"\n",
    "        Receives two numpy bi-dimentionals arrays and returns the accuracy of the model\n",
    "        \"\"\"\n",
    "        self.get_distances(test)\n",
    "        self.set_labels()\n",
    "        return self.get_accuracies(test)\n",
    "    \n",
    "    def suggestions(self, test, n_codes):\n",
    "        self.get_distances(test)\n",
    "        self.set_labels()\n",
    "        return pd.DataFrame(\n",
    "            np.concatenate([\n",
    "                np.reshape(test[:, 300], (-1, 1)), \n",
    "                np.reshape(test[:, 301], (-1, 1)), \n",
    "                self.topKS.iloc[:, :n_codes]],\n",
    "                axis=1\n",
    "                ), \n",
    "            columns=['Concept', 'Description']+['top-{} suggestion'.format(i+1) for i in range(n_codes)]\n",
    "            )\n",
    "        \n",
    "    def get_inertia(self): \n",
    "        \"\"\"\n",
    "        Return the inertia of the current model\n",
    "        \"\"\"\n",
    "        return self.km.inertia_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Codification Suggested](#codification-suggested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train process time: 1.742814064025879\n",
      "Clasification process time: 0.09805965423583984\n"
     ]
    }
   ],
   "source": [
    "# Instance of the model\n",
    "proc = AC_PLT(n_clusters=numberCluster)\n",
    "\n",
    "# Training process\n",
    "start = time.time()\n",
    "proc.fit(data_matrix)\n",
    "end = time.time()\n",
    "print(\"Train process time:\", end-start)\n",
    "\n",
    "# Creation of Dataframe with the suggested codes\n",
    "start = time.time()\n",
    "df = proc.suggestions(matrix_to_code, n_codes=numberCodes)\n",
    "end = time.time()\n",
    "print(\"Clasification process time:\", end-start)\n",
    "\n",
    "# Saving the dataframe into a csv file\n",
    "df.to_csv(outputFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "39d3428004f928037450bcaf2ccddacfbb5d68c6243c2762d0ea346348cf1412"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
