{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPN generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cosas', 'pronto', 'sean', 'aun', 'empleas', 'tiempo', 'llegó', 'trabajar', 'queremos', 'verdad', 'habrá', 'aquél', 'pero', 'cuál', 'más', 'primeros', 'trabajo', 'veces', 'trabajais', 'sola', 'hablan', 'mio', 'dan', 'excepto', 'ella', 'menos', 'podrias', 'durante', 'manera', 'cuanta', 'sigue', 'mías', 'dice', 'debido', 'informó', 'señaló', 'ultimo', 'nuestros', 'sería', 'intentar', 'deben', 'se', 'entre', 'considera', 'esa', 'estoy', 'mia', 'mía', 'cuantos', 'esta', 'haciendo', 'vaya', 'ninguno', 'tercera', 'cuatro', 'además', 'soyos', 'ambos', 'tambien', 'ellas', 'llevar', 'propias', 'despacio', 'bastante', 'podriais', 'muy', 'quien', 'empleo', 'pueda', 'conseguimos', 'podriamos', 'podrán', 'le', 'saben', 'sobre', 'poner', 'aún', 'usan', 'parte', 'tendrán', 'tengo', 'sera', 'da', 'eras', 'van', 'nueva', 'ademas', 'expresó', 'míos', 'seis', 'hicieron', 'sabe', 'eramos', 'hubo', 'lugar', 'propio', 'usar', 'nuestra', 'dar', 'que', 'apenas', 'repente', 'vuestros', 'encuentra', 'fuera', 'días', 'alli', 'sabeis', 'fue', 'segundo', 'hemos', 'detras', 'ha', 'indicó', 'proximo', 'quienes', 'sí', 'solos', 'podemos', 'han', 'pocas', 'agregó', 'peor', 'enfrente', 'haya', 'lado', 'cierta', 'dos', 'parece', 'aseguró', 'sin', 'estaban', 'estas', 'tenemos', 'usted', 'igual', 'tal', 'dia', 'tampoco', 'verdadero', 'sé', 'era', 'no', 'breve', 'diferentes', 'éste', 'es', 'trata', 'tarde', 'momento', 'yo', 'todas', 'horas', 'afirmó', 'varios', 'ellos', 'hoy', 'ex', 'dijeron', 'solas', 'tras', 'usais', 'creo', 'tres', 'alrededor', 'tuvo', 'cuándo', 'haces', 'buen', 'dentro', 'dónde', 'vuestro', 'vais', 'tenía', 'primera', 'cuantas', 'quién', 'tener', 'tu', 'añadió', 'mis', 'de', 'gueno', 'medio', 'antaño', 'cuántos', 'verdadera', 'últimas', 'ahí', 'tenido', 'dias', 'si', 'ser', 'ya', 'toda', 'nuestro', 'siete', 'nosotros', 'primer', 'final', 'propia', 'adelante', 'encima', 'nada', 'mí', 'tus', 'pues', 'hace', 'hecho', 'los', 'sus', 'casi', 'sido', 'nuevo', 'diferente', 'esas', 'usamos', 'aquélla', 'intenta', 'partir', 'siempre', 'mayor', 'otro', 'cuántas', 'debajo', 'fui', 'estuvo', 'desde', 'alguno', 'tuyas', 'embargo', 'estamos', 'todavía', 'mismos', 'aquella', 'misma', 'nosotras', 'ciertas', 'tuyo', 'quiza', 'aquellas', 'estos', 'eso', 'contigo', 'hizo', 'debe', 'demás', 'sabemos', 'bueno', 'suya', 'delante', 'general', 'aqui', 'nos', 'aunque', 'algunas', 'pudo', 'mismas', 'posible', 'ver', 'cómo', 'intentas', 'quizá', 'cuales', 'dio', 'ése', 'pasada', 'emplean', 'somos', 'mejor', 'podría', 'otros', 'próximo', 'paìs', 'ese', 'decir', 'tendrá', 'tuya', 'mios', 'nuevas', 'mas', 'todavia', 'enseguida', 'saber', 'existen', 'del', 'él', 'propios', 'cada', 'podria', 'pocos', 'mencionó', 'consigo', 'buenas', 'informo', 'aquel', 'conseguir', 'su', 'soy', 'menudo', 'solo', 'luego', 'vamos', 'próximos', 'modo', 'un', 'largo', 'consigue', 'eres', 'quizás', 'total', 'cuando', 'nadie', 'consideró', 'ésa', 'podeis', 'realizado', 'trabaja', 'este', 'vuestra', 'respecto', 'mucho', 'realizar', 'hacer', 'cuánta', 'tenga', 'trabajamos', 'ningunos', 'podrian', 'intentan', 'porque', 'puede', 'suyo', 'ir', 'después', 'ahora', 'las', 'poder', 'todos', 'explicó', 'será', 'pasado', 'incluso', 'cinco', 'qué', 'tiene', 'fin', 'estará', 'cuenta', 'pais', 'asi', 'uno', 'como', 'sabes', 'unas', 'sólo', 'dijo', 'ejemplo', 'qeu', 'siendo', 'conocer', 'muchos', 'está', 'anterior', 'hacemos', 'temprano', 'ocho', 'voy', 'tienen', 'últimos', 'fuimos', 'aquéllas', 'cual', 'estan', 'me', 'mientras', 'dicen', 'la', 'sois', 'tan', 'vuestras', 'tú', 'usas', 'con', 'tuyos', 'ésta', 'haber', 'trabajas', 'siguiente', 'antano', 'ciertos', 'mias', 'sea', 'habían', 'intentamos', 'esto', 'para', 'dieron', 'quizas', 'allí', 'dejó', 'eran', 'hay', 'os', 'éstos', 'ésas', 'ésos', 'junto', 'ante', 'principalmente', 'cuanto', 'hacerlo', 'por', 'podrá', 'hasta', 'alguna', 'ti', 'aquellos', 'lleva', 'nuestras', 'dicho', 'estados', 'sino', 'adrede', 'lo', 'bien', 'aproximadamente', 'una', 'cuánto', 'mucha', 'éstas', 'salvo', 'hago', 'aquello', 'valor', 'buenos', 'estais', 'poco', 'contra', 'vosotros', 'primero', 'intento', 'uso', 'son', 'trabajan', 'despues', 'emplear', 'tanto', 'manifestó', 'podrían', 'segunda', 'cuáles', 'teneis', 'en', 'el', 'cualquier', 'poca', 'demasiado', 'entonces', 'mío', 'mal', 'donde', 'arriba', 'esos', 'consigues', 'nunca', 'acuerdo', 'así', 'supuesto', 'ustedes', 'estar', 'va', 'cerca', 'había', 'ningún', 'ahi', 'puedo', 'habla', 'quiénes', 'ningunas', 'unos', 'actualmente', 'nuevos', 'todo', 'raras', 'algún', 'según', 'serán', 'día', 'antes', 'ni', 'bajo', 'estado', 'mediante', 'algunos', 'dado', 'están', 'haceis', 'les', 'suyas', 'otras', 'te', 'gran', 'pueden', 'habia', 'muchas', 'quiere', 'mi', 'he', 'usa', 'grandes', 'claro', 'hacia', 'aquéllos', 'también', 'cierto', 'empleais', 'ayer', 'través', 'al', 'estaba', 'intentais', 'fueron', 'hacen', 'realizó', 'segun', 'atras', 'quedó', 'última', 'varias', 'arribaabajo', 'comentó', 'buena', 'otra', 'detrás', 'algo', 'ninguna', 'solamente', 'conmigo', 'vez', 'mismo', 'último', 'consiguen', 'aquí', 'existe', 'ampleamos', 'vosotras', 'deprisa', 'pesar', 'ello', 'lejos'}\n"
     ]
    }
   ],
   "source": [
    "# Definitivamente mostrar al profe y ver que se hace con esto\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones Importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(text):\n",
    "    \"\"\"\n",
    "    Recive a string and return the same string without punctions and \n",
    "    stop words\n",
    "    \"\"\"\n",
    "     # Create a object spacy type nlp\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Remove all special caracters \n",
    "    words = [t.orth_ for t in doc if not (t.is_punct or t.is_stop)]\n",
    "    tokens = [t.lower() for t in words]\n",
    "\n",
    "    return(\" \".join(tokens))\n",
    "\n",
    "def Lematize(text):\n",
    "    \"\"\"\n",
    "    Recive a string and return the string Lematized\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Remove all special caracters \n",
    "    lemas = [t.lemma_ for t in tokens]\n",
    "\n",
    "    return(\" \".join(lemas))\n",
    "\n",
    "def to_vector(texto,model):\n",
    "    \"\"\" \n",
    "    Receives a sentence string along with a word embedding model and \n",
    "    returns the vector representation of the sentence\n",
    "    \"\"\"\n",
    "    tokens = texto.split() # splits the text by space and returns a list of words\n",
    "    vec = np.zeros(300) # creates an empty vector of 300 dimensions\n",
    "    for word in tokens: # iterates over the sentence\n",
    "        if word in model: # checks if the word is both in the word embedding\n",
    "            vec += model[word] # adds every word embedding to the vector\n",
    "    return vec / np.linalg.norm(vec) # divides the vector by their normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hola a'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Normalize(\"Hola a todos los últimos!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None # Para evitar que se muestre warnings\n",
    "# Normalizacion de los conceptos, descripciones y codificaciones\n",
    "for i in range(len(data)):\n",
    "    data['Palabra (concepto)'][i] = Normalize(data['Palabra (concepto)'][i])\n",
    "    data['Descripción'][i] = Normalize(data['Descripción'][i]) \n",
    "    data['Codificación'][i] = Normalize(data['Codificación'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? Puede ser el preprocesamiento algo importante que mostrar en mi tesis?\n",
    "# ## Analysis Frequency Distribution of Word Counts in Documents\n",
    "\n",
    "# doc_lens = [len(d) for d in df['sentence_clean']]\n",
    "\n",
    "# #plot\n",
    "# plt.figure(figsize=(16,7), dpi=160)\n",
    "# plt.hist(doc_lens, bins = 200, color='navy')\n",
    "# plt.text(150, 230, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "# plt.text(150,  200, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "# plt.text(150,  170, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "# plt.text(150,  140, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "# plt.text(150,  110, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "# plt.gca().set(xlim=(0, 200), ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "# plt.tick_params(size=16)\n",
    "# plt.xticks(np.linspace(0,200,9))\n",
    "# plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de Matriz de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the matrix to make the clustering process\n",
    "descriptions_matrix = np.zeros( # creatign an empty matrix\n",
    "    (\n",
    "        len(data), # the number of data points\n",
    "        len(model['hola']) # the number of components of the word embedding\n",
    "    )\n",
    ")\n",
    "# matrix filling \n",
    "for i,description in enumerate(data['Descripción']):\n",
    "    vector = to_vector(description,model)\n",
    "    descriptions_matrix[i,] = vector\n",
    "\n",
    "# Concatenate the matrix with the data of each observation\n",
    "data_matrix = np.concatenate([descriptions_matrix,data], axis=1)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# This is the important matrix\n",
    "# Remove of the 'Nan' data\n",
    "data_matrix_without_nan = data_matrix[~pd.isnull(data_matrix[:,:300]).any(axis=1)]\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# reduce the matrix to the importan\n",
    "data_matrix_without_nan = np.concatenate(\n",
    "    (data_matrix_without_nan[:,:300],                       # Vector of the description\n",
    "    np.reshape(data_matrix_without_nan[:,302], (-1, 1)),    # Cue/Concept\n",
    "    np.reshape(data_matrix_without_nan[:,306], (-1, 1)),    # Codification\n",
    "    np.reshape(data_matrix_without_nan[:,303], (-1, 1))     # Description of the cue\n",
    "    ),\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import\n",
    "data = pd.read_excel(r'../data/Datos Codificados Estudio v2a.xlsx')\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format( \n",
    "    r'../Data/SBW-vectors-300-min5.bin', # using the spanish billion words embeddings\n",
    "    binary=True # the model is in binary format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion de spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "# nlp.Defaults.stop_words.add(\"my_new_stopword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cosas', 'pronto', 'sean', 'aun', 'empleas', 'tiempo', 'llegó', 'trabajar', 'queremos', 'verdad', 'habrá', 'aquél', 'pero', 'cuál', 'más', 'primeros', 'trabajo', 'veces', 'trabajais', 'sola', 'hablan', 'mio', 'dan', 'excepto', 'ella', 'menos', 'podrias', 'durante', 'manera', 'cuanta', 'sigue', 'mías', 'dice', 'debido', 'informó', 'señaló', 'ultimo', 'nuestros', 'sería', 'intentar', 'deben', 'se', 'entre', 'considera', 'esa', 'estoy', 'mia', 'mía', 'cuantos', 'esta', 'haciendo', 'vaya', 'ninguno', 'tercera', 'cuatro', 'además', 'soyos', 'ambos', 'tambien', 'ellas', 'llevar', 'propias', 'despacio', 'bastante', 'podriais', 'muy', 'quien', 'empleo', 'pueda', 'conseguimos', 'podriamos', 'podrán', 'le', 'saben', 'sobre', 'poner', 'aún', 'usan', 'parte', 'tendrán', 'tengo', 'sera', 'da', 'eras', 'van', 'nueva', 'ademas', 'expresó', 'míos', 'seis', 'hicieron', 'sabe', 'eramos', 'hubo', 'lugar', 'propio', 'usar', 'nuestra', 'dar', 'que', 'apenas', 'repente', 'vuestros', 'encuentra', 'fuera', 'días', 'alli', 'sabeis', 'fue', 'segundo', 'hemos', 'detras', 'ha', 'indicó', 'proximo', 'quienes', 'sí', 'solos', 'podemos', 'han', 'pocas', 'agregó', 'peor', 'enfrente', 'haya', 'lado', 'cierta', 'dos', 'parece', 'aseguró', 'sin', 'estaban', 'estas', 'tenemos', 'usted', 'igual', 'tal', 'dia', 'tampoco', 'verdadero', 'sé', 'era', 'no', 'breve', 'diferentes', 'éste', 'es', 'trata', 'tarde', 'momento', 'yo', 'todas', 'horas', 'afirmó', 'varios', 'ellos', 'hoy', 'ex', 'dijeron', 'solas', 'tras', 'usais', 'creo', 'tres', 'alrededor', 'tuvo', 'cuándo', 'haces', 'buen', 'dentro', 'dónde', 'vuestro', 'vais', 'tenía', 'primera', 'cuantas', 'quién', 'tener', 'tu', 'añadió', 'mis', 'de', 'gueno', 'medio', 'antaño', 'cuántos', 'verdadera', 'últimas', 'ahí', 'tenido', 'dias', 'si', 'ser', 'ya', 'toda', 'nuestro', 'siete', 'nosotros', 'primer', 'final', 'propia', 'adelante', 'encima', 'nada', 'mí', 'tus', 'pues', 'hace', 'hecho', 'los', 'sus', 'casi', 'sido', 'nuevo', 'diferente', 'esas', 'usamos', 'aquélla', 'intenta', 'partir', 'siempre', 'mayor', 'otro', 'cuántas', 'debajo', 'fui', 'estuvo', 'desde', 'alguno', 'tuyas', 'embargo', 'estamos', 'todavía', 'mismos', 'aquella', 'misma', 'nosotras', 'ciertas', 'tuyo', 'quiza', 'aquellas', 'estos', 'eso', 'contigo', 'hizo', 'debe', 'demás', 'sabemos', 'bueno', 'suya', 'delante', 'general', 'aqui', 'nos', 'aunque', 'algunas', 'pudo', 'mismas', 'posible', 'ver', 'cómo', 'intentas', 'quizá', 'cuales', 'dio', 'ése', 'pasada', 'emplean', 'somos', 'mejor', 'podría', 'otros', 'próximo', 'paìs', 'ese', 'decir', 'tendrá', 'tuya', 'mios', 'nuevas', 'mas', 'todavia', 'enseguida', 'saber', 'existen', 'del', 'él', 'propios', 'cada', 'podria', 'pocos', 'mencionó', 'consigo', 'buenas', 'informo', 'aquel', 'conseguir', 'su', 'soy', 'menudo', 'solo', 'luego', 'vamos', 'próximos', 'modo', 'un', 'largo', 'consigue', 'eres', 'quizás', 'total', 'cuando', 'nadie', 'consideró', 'ésa', 'podeis', 'realizado', 'trabaja', 'este', 'vuestra', 'respecto', 'mucho', 'realizar', 'hacer', 'cuánta', 'tenga', 'trabajamos', 'ningunos', 'podrian', 'intentan', 'porque', 'puede', 'suyo', 'ir', 'después', 'ahora', 'las', 'poder', 'todos', 'explicó', 'será', 'pasado', 'incluso', 'cinco', 'qué', 'tiene', 'fin', 'estará', 'cuenta', 'pais', 'asi', 'uno', 'como', 'sabes', 'unas', 'sólo', 'dijo', 'ejemplo', 'qeu', 'siendo', 'conocer', 'muchos', 'está', 'anterior', 'hacemos', 'temprano', 'ocho', 'voy', 'tienen', 'últimos', 'fuimos', 'aquéllas', 'cual', 'estan', 'me', 'mientras', 'dicen', 'la', 'sois', 'tan', 'vuestras', 'tú', 'usas', 'con', 'tuyos', 'ésta', 'haber', 'trabajas', 'siguiente', 'antano', 'ciertos', 'mias', 'sea', 'habían', 'intentamos', 'esto', 'para', 'dieron', 'quizas', 'allí', 'dejó', 'eran', 'hay', 'os', 'éstos', 'ésas', 'ésos', 'junto', 'ante', 'principalmente', 'cuanto', 'hacerlo', 'por', 'podrá', 'hasta', 'alguna', 'ti', 'aquellos', 'lleva', 'nuestras', 'dicho', 'estados', 'sino', 'adrede', 'lo', 'bien', 'aproximadamente', 'una', 'cuánto', 'mucha', 'éstas', 'salvo', 'hago', 'aquello', 'valor', 'buenos', 'estais', 'poco', 'contra', 'vosotros', 'primero', 'intento', 'uso', 'son', 'trabajan', 'despues', 'emplear', 'tanto', 'manifestó', 'podrían', 'segunda', 'cuáles', 'teneis', 'en', 'el', 'cualquier', 'poca', 'demasiado', 'entonces', 'mío', 'mal', 'donde', 'arriba', 'esos', 'consigues', 'nunca', 'acuerdo', 'así', 'supuesto', 'ustedes', 'estar', 'va', 'cerca', 'había', 'ningún', 'ahi', 'puedo', 'habla', 'quiénes', 'ningunas', 'unos', 'actualmente', 'nuevos', 'todo', 'raras', 'algún', 'según', 'serán', 'día', 'antes', 'ni', 'bajo', 'estado', 'mediante', 'algunos', 'dado', 'están', 'haceis', 'les', 'suyas', 'otras', 'te', 'gran', 'pueden', 'habia', 'muchas', 'quiere', 'mi', 'he', 'usa', 'grandes', 'claro', 'hacia', 'aquéllos', 'también', 'cierto', 'empleais', 'ayer', 'través', 'al', 'estaba', 'intentais', 'fueron', 'hacen', 'realizó', 'segun', 'atras', 'quedó', 'última', 'varias', 'arribaabajo', 'comentó', 'buena', 'otra', 'detrás', 'algo', 'ninguna', 'solamente', 'conmigo', 'vez', 'mismo', 'último', 'consiguen', 'aquí', 'existe', 'ampleamos', 'vosotras', 'deprisa', 'pesar', 'ello', 'lejos'}\n"
     ]
    }
   ],
   "source": [
    "# Definitivamente mostrar al profe y ver que se hace con esto\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(text):\n",
    "    \"\"\"\n",
    "    Recive a string and return the same string without punctions and \n",
    "    stop words\n",
    "    \"\"\"\n",
    "     # Create a object spacy type nlp\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Remove all special caracters \n",
    "    words = [t.orth_ for t in doc if not (t.is_punct or t.is_stop)]\n",
    "    tokens = [t.lower() for t in words]\n",
    "\n",
    "    return(\" \".join(tokens))\n",
    "\n",
    "def Lematize(text):\n",
    "    \"\"\"\n",
    "    Recive a string and return the string Lematized\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Remove all special caracters \n",
    "    lemas = [t.lemma_ for t in tokens]\n",
    "\n",
    "    return(\" \".join(lemas))\n",
    "\n",
    "def to_vector(texto,model):\n",
    "    \"\"\" \n",
    "    Receives a sentence string along with a word embedding model and \n",
    "    returns the vector representation of the sentence\n",
    "    \"\"\"\n",
    "    tokens = texto.split() # splits the text by space and returns a list of words\n",
    "    vec = np.zeros(300) # creates an empty vector of 300 dimensions\n",
    "    for word in tokens: # iterates over the sentence\n",
    "        if word in model: # checks if the word is both in the word embedding\n",
    "            vec += model[word] # adds every word embedding to the vector\n",
    "    return vec / np.linalg.norm(vec) # divides the vector by their normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hola a'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Normalize(\"Hola a todos los últimos!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None # Para evitar que se muestre warnings\n",
    "# Normalizacion de los conceptos, descripciones y codificaciones\n",
    "for i in range(len(data)):\n",
    "    data['Palabra (concepto)'][i] = Normalize(data['Palabra (concepto)'][i])\n",
    "    data['Descripción'][i] = Normalize(data['Descripción'][i]) \n",
    "    data['Codificación'][i] = Normalize(data['Codificación'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the matrix to make the clustering process\n",
    "descriptions_matrix = np.zeros( # creatign an empty matrix\n",
    "    (\n",
    "        len(data), # the number of rows is equal to the number of data points\n",
    "        len(model['hola']) # the number of columns is equal to the number of components of the word embedding\n",
    "    )\n",
    ")\n",
    "# matrix filling \n",
    "for i,description in enumerate(data['Descripción']):\n",
    "    vector = to_vector(description,model)\n",
    "    descriptions_matrix[i,] = vector\n",
    "\n",
    "# Concatenate the matrix with the data of each observation\n",
    "data_matrix = np.concatenate([descriptions_matrix,data], axis=1)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# This is the important matrix\n",
    "# Remove of the 'Nan' data\n",
    "data_matrix_without_nan = data_matrix[~pd.isnull(data_matrix[:,:300]).any(axis=1)]\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# reduce the matrix to the importan\n",
    "data_matrix_without_nan = np.concatenate(\n",
    "    (data_matrix_without_nan[:,:300],                       # Vector of the description\n",
    "    np.reshape(data_matrix_without_nan[:,302], (-1, 1)),    # Cue/Concept\n",
    "    np.reshape(data_matrix_without_nan[:,306], (-1, 1)),    # Codification\n",
    "    np.reshape(data_matrix_without_nan[:,303], (-1, 1))     # Description of the cue\n",
    "    ),\n",
    "    axis = 1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fae2177e18fec49484b61032129f0d52bd80ea0aed801b0ea42284277d6347b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
