{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPN generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import gensim\n",
    "from collections import Counter \n",
    "import sklearn.cluster\n",
    "import random\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import\n",
    "data = pd.read_csv(r'../../data/nueva_data/propiedades_codigo.csv', delimiter=\"\\t\")\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format( \n",
    "    r'../../data/SBW-vectors-300-min5.bin.gz', # using the spanish billion words embeddings\n",
    "    binary=True # the model is in binary format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concepto</th>\n",
       "      <th>Respuesta</th>\n",
       "      <th>Codigo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>granito</td>\n",
       "      <td>tierra</td>\n",
       "      <td>terrestre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>granito</td>\n",
       "      <td>rugoso</td>\n",
       "      <td>texturas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>granito</td>\n",
       "      <td>construcción</td>\n",
       "      <td>material_construccion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>granito</td>\n",
       "      <td>desagradable al tacto</td>\n",
       "      <td>texturas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>granito</td>\n",
       "      <td>raspa</td>\n",
       "      <td>lastimar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Concepto              Respuesta                 Codigo\n",
       "0  granito                 tierra              terrestre\n",
       "1  granito                 rugoso               texturas\n",
       "2  granito           construcción  material_construccion\n",
       "3  granito  desagradable al tacto               texturas\n",
       "4  granito                  raspa               lastimar"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion de spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "# nlp.Defaults.stop_words.add(\"my_new_stopword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones Importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Recive a string and return the string in tokens without punctuations\n",
    "    and in lowercase\n",
    "    \"\"\"\n",
    "    # for each token in the sentence add to the list if is not a punctuation\n",
    "    return [t for t in nlp(text.lower()) if not t.is_punct]\n",
    "\n",
    "\n",
    "def normalize(tokens):\n",
    "    \"\"\"\n",
    "    Recive al list of string and return in one string without stop words\n",
    "    \"\"\"\n",
    "    # for each token if is not a stop word add the word to the list\n",
    "    words = [t.orth_ for t in tokens if not t.is_stop]\n",
    "\n",
    "    # return the tokens in one string\n",
    "    return(\" \".join(words))\n",
    "\n",
    "\n",
    "def lematize(tokens):\n",
    "    \"\"\"\n",
    "    Recive al list of string and return in one string without stop words \n",
    "    and Lematized\n",
    "    \"\"\"\n",
    "    # for each token if is not a stop word add the lemma of the word in the list\n",
    "    lemmas = [t.lemma_ for t in tokens if not t.is_stop]\n",
    "\n",
    "    # return the tokens in one string\n",
    "    return(\" \".join(lemmas))\n",
    "\n",
    "def to_vector(texto,model):\n",
    "    \"\"\" \n",
    "    Receives a sentence string along with a word embedding model and \n",
    "    returns the vector representation of the sentence\n",
    "    \"\"\"\n",
    "    tokens = texto.split() # splits the text by space and returns a list of words\n",
    "    vec = np.zeros(300) # creates an empty vector of 300 dimensions\n",
    "    for word in tokens: # iterates over the sentence\n",
    "        if word in model: # checks if the word is both in the word embedding\n",
    "            vec += model[word] # adds every word embedding to the vector\n",
    "    return vec / np.linalg.norm(vec) # divides the vector by their normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None # Para evitar que se muestre warnings\n",
    "\n",
    "# Normalizacion de los conceptos, descripciones y codificaciones\n",
    "for i in range(len(data)):\n",
    "    data['Respuesta'][i] = normalize(clean_text(data['Respuesta'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de Matriz de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dra98\\AppData\\Local\\Temp\\ipykernel_12832\\2310962255.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return vec / np.linalg.norm(vec) # divides the vector by their normal\n"
     ]
    }
   ],
   "source": [
    "# Creation of the matrix to make the clustering process\n",
    "descriptions_matrix = np.zeros( # creatign an empty matrix\n",
    "    (\n",
    "        len(data), # the number of data points\n",
    "        len(model['hola']) # the number of components of the word embedding\n",
    "    )\n",
    ")\n",
    "\n",
    "# matrix filling \n",
    "for i,description in enumerate(data['Respuesta']):\n",
    "    vector = to_vector(description,model)\n",
    "    descriptions_matrix[i,] = vector\n",
    "\n",
    "# Concatenate the matrix with the data of each observation\n",
    "data_matrix = np.concatenate([descriptions_matrix,data], axis=1)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# This is the important matrix\n",
    "# Remove of the 'Nan' data\n",
    "data_matrix_without_nan = data_matrix[~pd.isnull(data_matrix[:,:300]).any(axis=1)]\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# reduce the matrix to the importan\n",
    "data_matrix_without_nan = np.concatenate(\n",
    "    (data_matrix_without_nan[:,:300],                       # Vector of the description\n",
    "    np.reshape(data_matrix_without_nan[:,300], (-1, 1)),    # Cue/Concept\n",
    "    np.reshape(data_matrix_without_nan[:,302], (-1, 1)),    # Codification\n",
    "    np.reshape(data_matrix_without_nan[:,301], (-1, 1))     # Description of the cue\n",
    "    ),\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing random generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of test-train data\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold=KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000846629762292413\n",
      "0.00047635098789735567\n"
     ]
    }
   ],
   "source": [
    "rand_acc = []\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "    \n",
    "    train = data_matrix_without_nan[train_index,:]\n",
    "    test = data_matrix_without_nan[test_index,:]\n",
    "\n",
    "    train_codes=np.unique(train[:,301])\n",
    "\n",
    "    len_test = len(test)\n",
    "    counter = 0\n",
    "    for i in range(len_test):\n",
    "        counter += test[i,301] == np.random.choice(train_codes)\n",
    "    rand_acc.append(counter/len_test)\n",
    "\n",
    "print(np.mean(rand_acc))\n",
    "print(np.std(rand_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0020514490394008466\n",
      "0.00044409578954691806\n"
     ]
    }
   ],
   "source": [
    "rand_acc_top3 = []\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "    \n",
    "    train = data_matrix_without_nan[train_index,:]\n",
    "    test = data_matrix_without_nan[test_index,:]\n",
    "\n",
    "    train_codes=np.unique(train[:,301])\n",
    "\n",
    "    len_test = len(test)\n",
    "    counter = 0\n",
    "    for i in range(len_test):\n",
    "        for _ in range(3):\n",
    "            val = test[i,301] == np.random.choice(train_codes)\n",
    "            if val:\n",
    "                counter += val\n",
    "                break\n",
    "    rand_acc_top3.append(counter/len_test)\n",
    "\n",
    "print(np.mean(rand_acc_top3))\n",
    "print(np.std(rand_acc_top3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0027678280690328883\n",
      "0.0005545225127296126\n"
     ]
    }
   ],
   "source": [
    "rand_acc_top5 = []\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "    \n",
    "    train = data_matrix_without_nan[train_index,:]\n",
    "    test = data_matrix_without_nan[test_index,:]\n",
    "\n",
    "    train_codes=np.unique(train[:,301])\n",
    "\n",
    "    len_test = len(test)\n",
    "    counter = 0\n",
    "    for i in range(len_test):\n",
    "        for _ in range(5):\n",
    "            val = test[i,301] == np.random.choice(train_codes)\n",
    "            if val:\n",
    "                counter += val\n",
    "                break\n",
    "    rand_acc_top5.append(counter/len_test)\n",
    "\n",
    "print(np.mean(rand_acc_top5))\n",
    "print(np.std(rand_acc_top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0056007815043959625\n",
      "0.001116667417745506\n"
     ]
    }
   ],
   "source": [
    "rand_acc_top10 = []\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "    \n",
    "    train = data_matrix_without_nan[train_index,:]\n",
    "    test = data_matrix_without_nan[test_index,:]\n",
    "\n",
    "    train_codes=np.unique(train[:,301])\n",
    "\n",
    "    len_test = len(test)\n",
    "    counter = 0\n",
    "    for i in range(len_test):\n",
    "        for _ in range(10):\n",
    "            val = test[i,301] == np.random.choice(train_codes)\n",
    "            if val:\n",
    "                counter += val\n",
    "                break\n",
    "    rand_acc_top10.append(counter/len_test)\n",
    "\n",
    "print(np.mean(rand_acc_top10))\n",
    "print(np.std(rand_acc_top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030608922175187236\n",
      "0.0017199755970360474\n"
     ]
    }
   ],
   "source": [
    "rand_acc_top50 = []\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "    \n",
    "    train = data_matrix_without_nan[train_index,:]\n",
    "    test = data_matrix_without_nan[test_index,:]\n",
    "\n",
    "    train_codes=np.unique(train[:,301])\n",
    "\n",
    "    len_test = len(test)\n",
    "    counter = 0\n",
    "    for i in range(len_test):\n",
    "        for _ in range(50):\n",
    "            val = test[i,301] == np.random.choice(train_codes)\n",
    "            if val:\n",
    "                counter += val\n",
    "                break\n",
    "    rand_acc_top50.append(counter/len_test)\n",
    "\n",
    "print(np.mean(rand_acc_top50))\n",
    "print(np.std(rand_acc_top50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013741452295669162\n",
      "0.0005310812904689056\n"
     ]
    }
   ],
   "source": [
    "rand_concept_acc = []\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "\n",
    "    train = data_matrix_without_nan[train_index,:]\n",
    "    test = data_matrix_without_nan[test_index,:]\n",
    "\n",
    "    concept_cluster = {\n",
    "                i:np.unique(train[:,301][train[:,300]==i]) for i in np.unique(train[:,300])\n",
    "                }\n",
    "    \n",
    "    len_test = len(test)\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len_test):\n",
    "        counter += test[i,301] == np.random.choice(concept_cluster[test[i,300]])\n",
    "    rand_concept_acc.append(counter/len_test)\n",
    "\n",
    "print(np.mean(rand_concept_acc))\n",
    "print(np.std(rand_concept_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041322044936502766\n",
      "0.0019791040571830345\n"
     ]
    }
   ],
   "source": [
    "rand_concept_acc_top3 = []\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "\n",
    "    train = data_matrix_without_nan[train_index,:]\n",
    "    test = data_matrix_without_nan[test_index,:]\n",
    "\n",
    "    concept_cluster = {\n",
    "                i:np.unique(train[:,301][train[:,300]==i]) for i in np.unique(train[:,300])\n",
    "                }\n",
    "    \n",
    "    len_test = len(test)\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len_test):\n",
    "        for _ in range(3):\n",
    "            val = test[i,301] == np.random.choice(concept_cluster[test[i,300]])\n",
    "            if val:\n",
    "                counter += val\n",
    "                break\n",
    "    rand_concept_acc_top3.append(counter/len_test)\n",
    "\n",
    "print(np.mean(rand_concept_acc_top3))\n",
    "print(np.std(rand_concept_acc_top3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03995441224356887\n",
      "0.0029569354828385525\n"
     ]
    }
   ],
   "source": [
    "rand_concept_acc_top5 = []\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "\n",
    "    train = data_matrix_without_nan[train_index,:]\n",
    "    test = data_matrix_without_nan[test_index,:]\n",
    "\n",
    "    concept_cluster = {\n",
    "                i:np.unique(train[:,301][train[:,300]==i]) for i in np.unique(train[:,300])\n",
    "                }\n",
    "    \n",
    "    len_test = len(test)\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len_test):\n",
    "        for _ in range(3):\n",
    "            val = test[i,301] == np.random.choice(concept_cluster[test[i,300]])\n",
    "            if val:\n",
    "                counter += val\n",
    "                break\n",
    "    rand_concept_acc_top5.append(counter/len_test)\n",
    "\n",
    "print(np.mean(rand_concept_acc_top5))\n",
    "print(np.std(rand_concept_acc_top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04291761641159232\n",
      "0.00209669293806646\n"
     ]
    }
   ],
   "source": [
    "rand_concept_acc_top10 = []\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "\n",
    "    train = data_matrix_without_nan[train_index,:]\n",
    "    test = data_matrix_without_nan[test_index,:]\n",
    "\n",
    "    concept_cluster = {\n",
    "                i:np.unique(train[:,301][train[:,300]==i]) for i in np.unique(train[:,300])\n",
    "                }\n",
    "    \n",
    "    len_test = len(test)\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len_test):\n",
    "        for _ in range(3):\n",
    "            val = test[i,301] == np.random.choice(concept_cluster[test[i,300]])\n",
    "            if val:\n",
    "                counter += val\n",
    "                break\n",
    "    rand_concept_acc_top10.append(counter/len_test)\n",
    "\n",
    "print(np.mean(rand_concept_acc_top10))\n",
    "print(np.std(rand_concept_acc_top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041028980788016936\n",
      "0.0017322612931644926\n"
     ]
    }
   ],
   "source": [
    "rand_concept_acc_top50 = []\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "\n",
    "    train = data_matrix_without_nan[train_index,:]\n",
    "    test = data_matrix_without_nan[test_index,:]\n",
    "\n",
    "    concept_cluster = {\n",
    "                i:np.unique(train[:,301][train[:,300]==i]) for i in np.unique(train[:,300])\n",
    "                }\n",
    "    \n",
    "    len_test = len(test)\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len_test):\n",
    "        for _ in range(3):\n",
    "            val = test[i,301] == np.random.choice(concept_cluster[test[i,300]])\n",
    "            if val:\n",
    "                counter += val\n",
    "                break\n",
    "    rand_concept_acc_top50.append(counter/len_test)\n",
    "\n",
    "print(np.mean(rand_concept_acc_top50))\n",
    "print(np.std(rand_concept_acc_top50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class creation\n",
    "class PLT_processor:\n",
    "\n",
    "    def __init__(self, method = 0, n_clusters = 500):\n",
    "        \"\"\"\n",
    "        Recives the type of method to use, \n",
    "        - 0: Clasify using K-Means (Simple)\n",
    "        - 1: Clasify to the codification with the same concept \n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.method = method # Method of Processing the data\n",
    "        self.KMeans_dict = {}\n",
    "        self.KMeans_categories = {}\n",
    "\n",
    "        if method == 1:\n",
    "            self.inverse_concept_cluster = {}\n",
    "            self.concept_cluster = {}\n",
    "\n",
    "        self.topk = np.zeros((test.shape[0],500))\n",
    "\n",
    "    def most_frequent(self, List): \n",
    "        \"\"\"\n",
    "        Recives a list of words, and return the word most frequente of\n",
    "        the list\n",
    "        \"\"\"\n",
    "        # ? Agregar la frase 'PENDIENTE' en caso de que haya multiples codificaciones con la misma frecuencia ?\n",
    "        occurence_count = Counter(List) \n",
    "        return occurence_count.most_common(1)[0][0] \n",
    "\n",
    "\n",
    "    def model_train(self, train):\n",
    "        \"\"\"\n",
    "        Recives the train dataset and the number of clusters to train \n",
    "        the k-means model\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the number of clusters is less than the data points aply K Means\n",
    "        if(len(train)>self.n_clusters):\n",
    "            # creates de k-means object\n",
    "            self.kemean = sklearn.cluster.KMeans(n_clusters=self.n_clusters, random_state=0)\n",
    "            # Train the k-means algorithm\n",
    "            self.kemean.fit(train[:,:300])\n",
    "\n",
    "            # print(self.kemean.n_clusters)\n",
    "            # Dataframe of train dataset\n",
    "            df = pd.DataFrame(\n",
    "                np.concatenate([\n",
    "                    np.reshape(train[:,301], (-1, 1)),          # Human codification\n",
    "                    np.reshape(self.kemean.labels_, (-1, 1)),   # Number of the KMean centroid\n",
    "                    np.reshape(train[:,300], (-1, 1))           # Concept of the codification\n",
    "                    ], axis=1), \n",
    "                columns=['Human', 'KMeans', 'Concept'])\n",
    "        # Else if the number of clusters is equal than the data points set a number to each point\n",
    "        elif(len(train)==self.n_clusters):\n",
    "             # Dataframe of train dataset\n",
    "            df = pd.DataFrame(\n",
    "                np.concatenate([\n",
    "                    np.reshape(train[:,301], (-1, 1)),          # Human codification\n",
    "                    np.reshape(range(self.n_clusters), (-1, 1)),   # Number of the KMean centroid\n",
    "                    np.reshape(train[:,300], (-1, 1))           # Concept of the codification\n",
    "                    ], axis=1), \n",
    "                columns=['Human', 'KMeans', 'Concept'])\n",
    "\n",
    "            self.cluster_centers_2 = train[:,:300].astype(float)\n",
    "\n",
    "        # create a dictionary of all the humans codifications for each Cluster\n",
    "        self.KMeans_dict = df.groupby(by='KMeans')['Human'].apply(list).to_dict()\n",
    "\n",
    "        # Fill a dictionary with the most frecuent value in the centroid\n",
    "        for key, val in self.KMeans_dict.items():\n",
    "            self.KMeans_categories[key] = self.most_frequent(val)\n",
    "        \n",
    "        # Generates the prediction for the train dataset\n",
    "        df['KM_Prediction'] = df['KMeans'].map(self.KMeans_categories)\n",
    "\n",
    "        if self.method == 1:\n",
    "            # dictionary of the codifications that doesn't show in a concept\n",
    "            self.not_concept_cluster = {\n",
    "                i:[False if j in np.unique(df['KMeans'][df['Concept']==i]) else True for j in range(self.n_clusters)] \n",
    "                for i in np.unique(df['Concept'])\n",
    "                }\n",
    "            # dictionary of clusters for each concept\n",
    "            self.concept_cluster = {\n",
    "                i:np.unique(df['KMeans'][df['Concept']==i]) for i in np.unique(df['Concept'])\n",
    "                }\n",
    "        # return self.kemean.inertia_\n",
    "\n",
    "    \n",
    "    def get_distances(self, test):\n",
    "        \"\"\"\n",
    "        recives the test data to calculate the distances of each frase, return \n",
    "        a matrix with the distances sorted\n",
    "        \"\"\"\n",
    "\n",
    "        # Distance matrix of each test point to each cluster center\n",
    "        distance_matrix = np.zeros((test.shape[0],self.n_clusters))\n",
    "\n",
    "        if(len(train)>self.n_clusters):\n",
    "            # for each cluster center\n",
    "            for i in range(distance_matrix.shape[1]):    \n",
    "                # Euclidian distance of each point to the i-est cluster center\n",
    "                distance_matrix[:,i]=np.sqrt(np.sum((test[:,:300].astype(float)-self.kemean.cluster_centers_[i,:])**2,axis=1))\n",
    "        elif(len(train)==self.n_clusters):\n",
    "            for i in range(distance_matrix.shape[1]):    \n",
    "                # Euclidian distance of each point to the i-est cluster center\n",
    "                distance_matrix[:,i]=np.sqrt(np.sum((test[:,:300].astype(float)-self.cluster_centers_2[i,:])**2,axis=1))\n",
    "        \n",
    "\n",
    "        if self.method == 1:\n",
    "            # for each datapoint\n",
    "            for i in range(distance_matrix.shape[0]):\n",
    "                # if the centroid not share the same concept asing NaN value\n",
    "                distance_matrix[i, self.not_concept_cluster[test[i,300]]] = np.nan\n",
    "    \n",
    "\n",
    "        # Sorting distances\n",
    "        self.topk=np.argsort(distance_matrix,axis=1)\n",
    "\n",
    "    \n",
    "    def set_labels(self, test):\n",
    "        \"\"\"\n",
    "        Create a new matrix from the clusters sorted and change the value\n",
    "        from numeric to the string according the codification\n",
    "        \"\"\"\n",
    "\n",
    "        # Change of the numeric value to the codification \n",
    "        self.topKS=pd.DataFrame(self.topk)\n",
    "\n",
    "        # create a temporal array of the kmeans categories\n",
    "        tempData = np.array([value for (_, value) in sorted(self.KMeans_categories.items())])\n",
    "        \n",
    "        # for each cluster center\n",
    "        # print(\"Setting labels\")\n",
    "        # print('len temp',len(self.KMeans_categories))\n",
    "        # print('dim topKS',self.topKS.shape[1])\n",
    "        for j in range(self.topKS.shape[1]):\n",
    "            # set the codification of the numeric value in the topk list\n",
    "            self.topKS.iloc[:,j]=tempData[self.topk[:,j]]\n",
    "        \n",
    "        if self.method == 1:\n",
    "            # for each datapoint\n",
    "            for i in range(self.topKS.shape[0]):\n",
    "                # Remove of the clusters that not share the same concept\n",
    "                self.topKS.iloc[i, np.isin(self.topk[i,:], self.concept_cluster[test[i,300]] ,invert=True)]= np.nan\n",
    "\n",
    "\n",
    "    def get_accuracies(self, test):\n",
    "        \"\"\"\n",
    "        Recives the test matrix and return the accuracies of the \n",
    "        diferents predictions\n",
    "        \"\"\"\n",
    "        #Creating the accuracy table to check each data point\n",
    "        testLabel=np.zeros(self.topKS.shape)\n",
    "        indexes_method0=pd.DataFrame(np.zeros((self.topKS.shape[0],2)), columns=['index', 'value']) \n",
    "\n",
    "        #For each data point\n",
    "        for i in range(testLabel.shape[0]):\n",
    "            #Checking if some of the cluster is able to classify it right\n",
    "            boolClass=self.topKS.iloc[i,:]==test[i,301]\n",
    "            if sum(boolClass)>0:\n",
    "                getIndex=boolClass.idxmax()\n",
    "                indexes_method0.iloc[i,0] = getIndex\n",
    "                indexes_method0.iloc[i,1] = self.topKS.iloc[i,getIndex]\n",
    "                #Setting the rest of the data point as 1\n",
    "                testLabel[i,getIndex:]=1\n",
    "            else:\n",
    "                indexes_method0.iloc[i,0] = np.nan\n",
    "                indexes_method0.iloc[i,1] = np.nan\n",
    "        accuracies=testLabel.sum(axis=0)/testLabel.shape[0]\n",
    "\n",
    "        return accuracies\n",
    "\n",
    "    def process(self, train, test):\n",
    "        self.model_train(train)\n",
    "        self.get_distances(test)\n",
    "        self.set_labels(test)\n",
    "        return self.get_accuracies(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3640182351025724\n",
      "0.00933669286880043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold=KFold(n_splits=5)\n",
    "\n",
    "method0_acc = np.zeros(5)\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "    train = data_matrix_without_nan[train_index, :]\n",
    "    test = data_matrix_without_nan[test_index, :]\n",
    "\n",
    "    proc = PLT_processor(0)\n",
    "    acc = proc.process(train, test)\n",
    "\n",
    "    method0_acc[i] = acc[0]\n",
    "    i+=1\n",
    "\n",
    "print(method0_acc.mean())\n",
    "print(method0_acc.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41432758059264085\n",
      "0.006234202016922397\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold=KFold(n_splits=5)\n",
    "\n",
    "method0_acc_top3 = np.zeros(5)\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "    train = data_matrix_without_nan[train_index, :]\n",
    "    test = data_matrix_without_nan[test_index, :]\n",
    "\n",
    "    proc = PLT_processor(0)\n",
    "    acc = proc.process(train, test)\n",
    "\n",
    "    method0_acc_top3[i] = acc[2]\n",
    "    i+=1\n",
    "\n",
    "print(method0_acc_top3.mean())\n",
    "print(method0_acc_top3.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41432758059264085\n",
      "0.006234202016922397\n"
     ]
    }
   ],
   "source": [
    "print(method0_acc_top3.mean())\n",
    "print(method0_acc_top3.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 500\n",
    "# Top 1, 3, 5, 10, 50\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold=KFold(n_splits=5)\n",
    "k500_acc_top = np.zeros((5, 5))\n",
    "\n",
    "tops=(0,2,4,9,49)\n",
    "\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "    train = data_matrix_without_nan[train_index, :]\n",
    "    test = data_matrix_without_nan[test_index, :]\n",
    "\n",
    "    proc = PLT_processor(0)\n",
    "    acc = proc.process(train, test)\n",
    "\n",
    "    for j in range(len(tops)):\n",
    "        k500_acc_top[i,j] = acc[tops[j]]\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36401824 0.41432758 0.43523282 0.46193422 0.52725497]\n",
      "[0.00933669 0.0062342  0.00717074 0.00677852 0.00843342]\n"
     ]
    }
   ],
   "source": [
    "print(k500_acc_top.mean(axis=0))\n",
    "print(k500_acc_top.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 1000\n",
    "# Top 1, 3, 5, 10, 50\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold=KFold(n_splits=5)\n",
    "k1000_acc_top = np.zeros((5, 5))\n",
    "\n",
    "tops=(0,2,4,9,49)\n",
    "\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "    train = data_matrix_without_nan[train_index, :]\n",
    "    test = data_matrix_without_nan[test_index, :]\n",
    "\n",
    "    proc = PLT_processor(0, n_clusters=1000)\n",
    "    acc = proc.process(train, test)\n",
    "\n",
    "    for j in range(len(tops)):\n",
    "        k1000_acc_top[i,j] = acc[tops[j]]\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46610225 0.52132856 0.54343862 0.57284272 0.64649951]\n",
      "[0.00715816 0.00526972 0.00460437 0.00617198 0.00664183]\n"
     ]
    }
   ],
   "source": [
    "print(k1000_acc_top.mean(axis=0))\n",
    "print(k1000_acc_top.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 3827\n",
    "# Top 1, 3, 5, 10, 50\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold=KFold(n_splits=5)\n",
    "k3950_acc_top = np.zeros((5, 5))\n",
    "\n",
    "tops=(0,2,4,9,49)\n",
    "\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "    train = data_matrix_without_nan[train_index, :]\n",
    "    test = data_matrix_without_nan[test_index, :]\n",
    "\n",
    "    proc = PLT_processor(0, n_clusters=len(train_index))\n",
    "    acc = proc.process(train, test)\n",
    "\n",
    "    for j in range(len(tops)):\n",
    "        k3950_acc_top[i,j] = acc[tops[j]]\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k3950_acc_top.mean(axis=0))\n",
    "print(k3950_acc_top.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold=KFold(n_splits=5)\n",
    "\n",
    "method1_acc = np.zeros(5)\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "    train = data_matrix_without_nan[train_index, :]\n",
    "    test = data_matrix_without_nan[test_index, :]\n",
    "\n",
    "    proc = PLT_processor(1)\n",
    "    acc = proc.process(train, test)\n",
    "\n",
    "    method1_acc[i] = acc[0]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(method1_acc.mean())\n",
    "print(method1_acc.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busqueda del K optimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "rango = range(25,1301,25)\n",
    "k_inertia = np.zeros(len(rango))\n",
    "j = 0\n",
    "for k in rango:\n",
    "    kfold=KFold(n_splits=5)\n",
    "\n",
    "    temp = np.zeros(5)\n",
    "    i=0\n",
    "    for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "        train = data_matrix_without_nan[train_index, :]\n",
    "        test = data_matrix_without_nan[test_index, :]\n",
    "\n",
    "        proc = PLT_processor(0, k)\n",
    "        inertia = proc.model_train(train)\n",
    "\n",
    "        temp[i] = inertia\n",
    "        i+=1\n",
    "    k_inertia[j] = temp.mean()\n",
    "    j+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "\n",
    "(ggplot()+aes(x=rango, y=k_inertia)+geom_line())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "rango = range(25,1301,25)\n",
    "k_acc = np.zeros(len(rango))\n",
    "j = 0\n",
    "for k in rango:\n",
    "    kfold=KFold(n_splits=5)\n",
    "\n",
    "    temp = np.zeros(5)\n",
    "    i=0\n",
    "    for train_index, test_index in kfold.split(data_matrix_without_nan):\n",
    "        train = data_matrix_without_nan[train_index, :]\n",
    "        test = data_matrix_without_nan[test_index, :]\n",
    "\n",
    "        proc = PLT_processor(0, k)\n",
    "        acc = proc.process(train, test)\n",
    "\n",
    "        temp[i] = acc[0]\n",
    "        i+=1\n",
    "    k_acc[j] = temp.mean()\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot()+aes(x=rango, y=k_acc)+geom_line())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'../../data/nueva_data/propiedades_codigo.csv', delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEMATIZACION\n",
    "pd.options.mode.chained_assignment = None # Para evitar que se muestre warnings\n",
    "\n",
    "# Normalizacion de los conceptos, descripciones y codificaciones\n",
    "for i in range(len(data)):\n",
    "    data['Respuesta'][i] = lematize(clean_text(data['Respuesta'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dra98\\AppData\\Local\\Temp\\ipykernel_12832\\2310962255.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return vec / np.linalg.norm(vec) # divides the vector by their normal\n"
     ]
    }
   ],
   "source": [
    "# Creation of the matrix to make the clustering process\n",
    "descriptions_matrix_lema = np.zeros( # creatign an empty matrix\n",
    "    (\n",
    "        len(data), # the number of data points\n",
    "        len(model['hola']) # the number of components of the word embedding\n",
    "    )\n",
    ")\n",
    "\n",
    "# matrix filling \n",
    "for i,description in enumerate(data['Respuesta']):\n",
    "    vector = to_vector(description,model)\n",
    "    descriptions_matrix_lema[i,] = vector\n",
    "\n",
    "# Concatenate the matrix with the data of each observation\n",
    "data_matrix_lema = np.concatenate([descriptions_matrix_lema,data], axis=1)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# This is the important matrix\n",
    "# Remove of the 'Nan' data\n",
    "data_matrix_without_nan_lema = data_matrix_lema[~pd.isnull(data_matrix_lema[:,:300]).any(axis=1)]\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# reduce the matrix to the importan\n",
    "data_matrix_without_nan_lema = np.concatenate(\n",
    "    (data_matrix_without_nan_lema[:,:300],                       # Vector of the description\n",
    "    np.reshape(data_matrix_without_nan_lema[:,300], (-1, 1)),    # Cue/Concept\n",
    "    np.reshape(data_matrix_without_nan_lema[:,302], (-1, 1)),    # Codification\n",
    "    np.reshape(data_matrix_without_nan_lema[:,301], (-1, 1))     # Description of the cue\n",
    "    ),\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36922817 0.41354501 0.43043405 0.45640939 0.52116197]\n",
      "[0.01145839 0.01107725 0.01111222 0.00949254 0.01011054]\n"
     ]
    }
   ],
   "source": [
    "# K = 500\n",
    "# Top 1, 3, 5, 10, 50\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold=KFold(n_splits=5)\n",
    "k500_acc_top_lema = np.zeros((5, 5))\n",
    "\n",
    "tops=(0,2,4,9,49)\n",
    "\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan_lema):\n",
    "    train = data_matrix_without_nan_lema[train_index, :]\n",
    "    test = data_matrix_without_nan_lema[test_index, :]\n",
    "\n",
    "    proc = PLT_processor(0)\n",
    "    acc = proc.process(train, test)\n",
    "\n",
    "    for j in range(len(tops)):\n",
    "        k500_acc_top_lema[i,j] = acc[tops[j]]\n",
    "\n",
    "    i+=1\n",
    "\n",
    "print(k500_acc_top_lema.mean(axis=0))\n",
    "print(k500_acc_top_lema.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47748691 0.5289647  0.54818443 0.57449755 0.64678264]\n",
      "[0.00874872 0.00689851 0.00621774 0.00452374 0.00631262]\n"
     ]
    }
   ],
   "source": [
    "# K = 1000\n",
    "# Top 1, 3, 5, 10, 50\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold=KFold(n_splits=5)\n",
    "k1000_acc_top_lema = np.zeros((5, 5))\n",
    "\n",
    "tops=(0,2,4,9,49)\n",
    "\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan_lema):\n",
    "    train = data_matrix_without_nan_lema[train_index, :]\n",
    "    test = data_matrix_without_nan_lema[test_index, :]\n",
    "\n",
    "    proc = PLT_processor(0, n_clusters=1000)\n",
    "    acc = proc.process(train, test)\n",
    "\n",
    "    for j in range(len(tops)):\n",
    "        k1000_acc_top_lema[i,j] = acc[tops[j]]\n",
    "\n",
    "    i+=1\n",
    "\n",
    "print(k1000_acc_top_lema.mean(axis=0))\n",
    "print(k1000_acc_top_lema.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = N\n",
    "# Top 1, 3, 5, 10, 50\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold=KFold(n_splits=5)\n",
    "kn_acc_top_lema = np.zeros((5, 5))\n",
    "\n",
    "tops=(0,2,4,9,49)\n",
    "\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kfold.split(data_matrix_without_nan_lema):\n",
    "    train = data_matrix_without_nan_lema[train_index, :]\n",
    "    test = data_matrix_without_nan_lema[test_index, :]\n",
    "\n",
    "    proc = PLT_processor(0, n_clusters=len(train))\n",
    "    acc = proc.process(train, test)\n",
    "\n",
    "    for j in range(len(tops)):\n",
    "        kn_acc_top_lema[i,j] = acc[tops[j]]\n",
    "\n",
    "    i+=1\n",
    "\n",
    "print(kn_acc_top_lema.mean(axis=0))\n",
    "print(kn_acc_top_lema.std(axis=0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f86d5b59def65e88dbeda6389e0ae43ef80c1809fa447fa67ec7c4f0e393c674"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
